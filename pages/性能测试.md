- 性能领域、性能测试、容量测试、基准测试、性能分析
- 《性能之巅》整理
  collapsed:: true
	- [[系统性能]]
		- 系统性能是对整个系统的研究，包括了所有的硬件组件和整个软件栈。
		- 所有数据路径上和软硬件上所发生的事情都包括在内。
		- ![image.png](../assets/image_1646147314960_0.png){:height 341, :width 426}
	- 人员
		- 系统性能是一项需要多类人员参与的事务。
	- 事情
		- [[性能]]领域包括的事情（按理想顺序排列）
			- 1 设置性能目标和建立性能模型
			- 2 基于软件或硬件原型进行性能特征归纳
			- 3 对开发代码进行[[性能分析]](软件整合之前)
			- 4 执行软件非回归性测试 (软件发布前或发布后)
			- 5 针对软件发布版本的[[基准测试]]
			- 6 目标环境中的概念验证( Proof-of-concept )测试
			- 7 生产环境部署的配置优化
			- 8 监控生产环境中运行的软件
			- 9 特定问题的[[性能分析]]
		- 术语[[容量规划]]
			- 指的是一系列事前行动。在设计阶段，包括通过研究开发软件的资源占用情况，来得知原有设计在多大程度上能满足目标需求。在部署后,包括监控资源的使用情况，这样问题在出现之前就能被预测。
	- 视角
		- 两种性能分析的视角
			- [[负载分析]]（workload analysis）
			- [[资源分析]]（resource analysis）
		- ![image.png](../assets/image_1646147812645_0.png){:height 264, :width 445}
		- 系统管理员作为系统资源的负责人，通常采用[[资源分析]]视角。
		- 应用程序开发人员，对最终实现的负载性能负责，通常采用[[负载分析]]视角。
	- [[性能]]是充满挑战的
		- 性能是主观的
			- 开始着手性能问题的时候，对问题是否存在的判断都有可能是模糊的，在问题被修复的时候也同样。
			- 从某种程度上说，一个给定指标是“好”或“坏”取决于应用开发人员和最终用户的性能预期。
			- 通过定义清晰的目标，可以吧主观的性能变得客观化
				- 诸如目标平均响应时间、或者对落进一定响应延时范围内的请求统计其百分比。
		- 系统是[[复杂]]的
			- 组件的互联
				- 性能问题可能出在子系统之间复杂的互联上，即使这些子系统隔离时表现得都很好。
				- 也可能由于连锁故障（cascading failure）出现性能问题，这指的是一个出现故障的组件会导致其他组件产生性能问题。
			- 问题的关联
				- 修复一个问题可能只是把瓶颈推向了系统里的其他地方，导致整体性能没有提升。
			- 生产环境负载的复杂特性
				- 实验室环境很难重现这类情况，或者只能间歇式地重现。
			- 解决复杂的性能问题常常需要全局性的方法
				- 整个系统——包括自身内部和外部的交互——都可能需要被调查研究。
		- 可能有多个问题并存
			- 真正的任务不是寻找问题，而是辨别哪些问题是最重要的
			- 性能分析必须量化问题的重要程度。
	- [[量化]]性能的指标举例
		- 延时
			- 有一个指标非常适合用来量化性能，那就是延时( latency )。
			- 延时测量的是用于等待的时间。广义来说，它可以表示所有操作完成的耗时
			- 虽然延时是一个非常有用的指标，但也不是随时随地都能获得。
				- 某些系统只有平均延时，某些系统则完全没有延时指标。
				- [[动态跟踪]]( dynamic tracing )可以从任意感兴趣的点测量延时，还可以提供数据以显示延时完整的分布情况。
	- [[动态跟踪]]
		- 动态跟踪技术把所有的软件变得可以监控，而且能用在真实的生产环境中。
		- 这项技术利用内存中的CPU指令并在这些指令之上动态构建监测数据。
			- 这样从任何运行的软件中都可以获得定制化的性能统计数据，从而提供了远超系统的自带统计所能给予的观测性。
				- 从前因为不易观测而无法处理的问题变得可以解决。
				- 从前可以解决而难以解决的问题，现在也往往可以得以简化。
		- 举例
			- DTrace
				- 在DTrace之前，系统跟踪( system tracing )常常使用静态探针( static probes) :置于内核和其他软件之上的一小套监测点。这种方法的观测是有限的，一般用起来很费时，需要配置、跟踪、导出数据以及最后分析一整套流程。
				- DTrace对用户态和内核态的软件都提供了静态跟踪和动态跟踪，并且数据是实时产生的。
	- [[云计算]]
		- 给系统性能带来影响的最新进展来自云计算和云计算的根基——虚拟技术的兴起。
		- 云计算采用的架构能让应用程序均衡分布于数目不断增多的小型系统中，这让快速扩展成为可能。
			- 这种方法还降低了对[[容量规划]]的精确程度的要求，因为更多的容量可以很便捷地在云端添加。
		- 在某些情况下，它对[[性能分析]]的需求更高了
			- 使用较少的资源就意味着系统更少。
			  id:: 621f58d3-18bb-4579-b65b-320d7cd893b7
				- 云的使用通常是按小时计费的，性能的优势可以减少系统的使用数目，从而直接节约成本。这和企业用户的情况不同，企业用户被一个支持协议锁定数年，直到合同终结都可能无法实现成本的节约。
		- [[云计算]]和[[虚拟化]]技术也带来了新的难题，这包括
			- 如何管理其他[[租户]]( tenant,有时被称作性能隔离( performance isolation) )带来的性能影响，以及如何让每个租户都能对物理系统做观测。
				- 举个例子，除非系统管理得很好，否则磁盘I/O性能可能因为同邻近租户的竞争而下降。
			- 在某些环境中，并不是每一个租户都能观察到物理磁盘的真实使用情况，这让问题的甄别变得困难。
		-
- 软件测试52讲 性能测试篇
	- 解读不同视角的软件性能与性能指标
	  collapsed:: true
		- 当我们谈及[[软件性能]]的时候，我们到底谈的是什么？
			- 目前，对软件性能最普遍的理解就是软件处理的及时性。
			- 但其实，从不同的系统类型，以及不同的视角去讨论软件性能，都会有所区别。
				- 对于不同类型的系统，软件性能的关注点各不相同
					- Web 类应用和手机端应用，一般以终端用户感受到的端到端的响应时间来描述系统的性能；
					- 非交互式的应用，比如典型的电信和银行后台处理系统，响应时间关注更多的是事件处理的速度，以及单位时间的事件吞吐量。
				- 同样地，对同一个系统来说，不同的对象群体对软件性能的关注点和期望也不完全相同，甚至很多时候是对立的。
					- 这里，不同的对象群体可以分为四大类：终端用户、系统运维人员、软件设计开发人员和性能测试人员。
		- 四个角度的软件性能
			- 终端用户眼中的软件性能
				- 从终端用户（也就是软件系统使用者）的维度来讲，软件性能表现为用户进行业务操作时的主观响应时间。
					- 具体来讲就是，从用户在界面上完成一个操作开始，到系统把本次操作的结果以用户能察觉的方式展现出来的全部时间。
						- 对终端用户来说，这个时间越短体验越好
				- 这个响应时间是终端用户对系统性能的最直观印象，包括了系统响应时间和前端展现时间。
					- 系统响应时间
						- 反应的是系统能力，又可以进一步细分为应用系统处理时间、数据库处理时间和网络传输时间等；
					- 前端展现时间
						- 取决于用户端的处理能力。
			- 系统运维人员眼中的软件性能
				- 从软件系统运维（也就是系统运维人员）的角度，软件性能除了包括单个用户的响应时间外，更要关注
					- 大量用户并发访问时的负载，
					- 以及可能的更大负载情况下的系统健康状态、并发处理能力、
					- 当前部署的系统容量、
					- 可能的系统瓶颈、系统配置层面的调优、数据库的调优，
					- 以及长时间运行稳定性和可扩展性。
			- 软件设计开发人员眼中的软件性能
				- 从软件系统开发（也就是软件设计开发人员）的角度来讲，软件性能关注的是性能相关的设计和实现细节，这几乎涵盖了软件设计和开发的全过程。
				- 在软件设计开发人员眼中，软件性能通常会包含算法设计、架构设计、性能最佳实践、数据库相关、软件性能的可测试性这五大方面。
					- [[算法]]设计包含的点
						- 核心算法的设计与实现是否高效；
						- 必要时，设计上是否采用 buffer 机制以提高性能，降低 I/O；
						- 是否存在潜在的内存泄露；
						- 是否存在并发环境下的线程安全问题；
						- 是否存在不合理的线程同步方式；
						- 是否存在不合理的资源竞争。
					- 第二，[[架构]]设计包含的内容：
						- 站在整体系统的角度，是否可以方便地进行系统容量和性能扩展；
						- 应用集群的可扩展性是否经过测试和验证；
						- 缓存集群的可扩展性是否经过测试和验证；
						- 数据库的可扩展性是否经过测试和验证。
					- 第三，性能最佳实践包含的点：
						- 代码实现是否遵守开发语言的性能最佳实践；
						- 关键代码是否在白盒级别进行性能测试；
						- 是否考虑前端性能的优化；
						- 必要的时候是否采用数据压缩传输；
						- 对于既要压缩又要加密的场景，
						- 是否采用先压缩后加密的顺序。
					- 第四，数据库相关的点：
						- 数据库表设计是否高效；
						- 是否引入必要的索引；
						- SQL 语句的执行计划是否合理；
						- SQL 语句除了功能是否要考虑性能要求；
						- 数据库是否需要引入读写分离机制；
						- 系统冷启动后，缓存大量不命中的时候，数据库承载的压力是否超负荷。
					- 第五，软件性能的[[可测试性]]包含的点：
						- 是否为性能分析（Profiler）提供必要的接口支持；
						- 是否支持高并发场景下的性能打点；
						- 是否支持全链路的性能分析。
					-
				- 需要注意的是，软件开发人员一般不会关注系统部署级别的性能，
					- 比如软件运行目标操作系统的调优、应用服务器的参数调优、数据库的参数调优、网络环境的调优等。
					- 系统部署级别的性能测试，目前一般是在系统性能测试阶段或者系统容量规划阶段，由性能测试人员、系统架构师，以及数据库管理员（DBA）协作完成。
			- 性能测试人员眼中的软件性能
				- 从性能工程的角度看，性能测试工程师关注的是算法设计、架构设计、性能最佳实践、数据库相关、软件性能的可测试性这五大方面。
				- 在系统架构师、DBA，以及开发人员的协助下，性能测试人员既要能够准确把握软件的性能需求，又要能够准确定位引起“不好”性能表现的制约因素和根源，并提出相应的解决方案。
				- 一个优秀的性能测试工程师，一般需要具有以下技能：
					- 性能需求的总结和抽象能力；
					- 根据性能测试目标，精准的性能测试场景设计和计算能力；
					- 性能测试场景和性能测试脚本的开发和执行能力；
					- 测试性能报告的分析解读能力；
					- 性能瓶颈的快速排查和定位能力；
					- 性能测试数据的设计和实现能力；
					- 面对互联网产品，全链路压测的设计与执行能力，能够和系统架构师一起处理流量标记、影子数据库等的技术设计能力；
					- 深入理解性能测试工具的内部实现原理，当性能测试工具有限制时，可以进行扩展二次开发；
					- 极其宽广的知识面，既要有“面”的知识，比如系统架构、存储架构、网络架构等全局的知识，还要有大量“点”的知识积累，比如数据库 SQL 语句的执行计划调优、JVM 垃圾回收（GC）机制、多线程常见问题等等。
				-
		- 衡量软件性能的三个最常用的指标：并发用户数、响应时间，以及系统吞吐量
			- [[并发用户数]]
				- 并发用户数，是性能需求与测试最常用，也是最重要的指标之一。它包含了业务层面和后端服务器层面的两层含义。
					- 业务层面的并发用户数，指的是实际使用系统的用户总数。
						- 但是，单靠这个指标并不能反映系统实际承载的压力，我们还要结合用户行为模型才能得到系统实际承载的压力。
					- 后端服务器层面的并发用户数，指的是“同时向服务器发送请求的数量”，直接反映了系统实际承载的压力。
				- 获取用户行为模式的方法，主要分为两种：
					- 对于已经上线的系统来说，往往采用系统日志分析法获取用户行为统计和峰值并发量等重要信息；
					- 而对于未上线的全新系统来说，通常的做法是参考行业中类似系统的统计信息来建模，然后分析。
			- [[响应时间]]
				- 通俗来讲，响应时间反映了完成某个操作所需要的时间，其标准定义是“应用系统从请求发出开始，到客户端接收到最后一个字节数据所消耗的时间”，是用户视角软件性能的主要体现。
				- 响应时间，分为前端展现时间和系统响应时间两部分。
					- 其中，前端时间，又称呈现时间，取决于客户端收到服务器返回的数据后渲染页面所消耗的时间；
					- 而系统响应时间，又可以进一步划分为 Web 服务器时间、应用服务器时间、数据库时间，以及各服务器间通信的网络时间。
				- 除非是针对前端的性能测试与调优，软件的性能测试一般更关注服务器端。
					- 但是，服务器端响应时间的概念非常清晰、直接，就是指从发出请求起到处理完成的时间，没有二义性；
					- 而前端时间的定义，在我看来存在些歧义。
						- 虽然前端时间一定程度上取决于客户端的处理能力，但是前端开发人员现在还会使用一些编程技巧在数据尚未完全接收完成时呈现数据，以减少用户实际感受到的主观响应时间。
						- 也就是说，我们现在会普遍采用提前渲染技术，使得用户实际感受到的响应时间通常要小于标准定义的响应时间
				- 严格来讲，响应时间应该包含两层含义：
					- 技术层面的标准定义和基于用户主观感受时间的定义。
					- 而在性能测试过程中，我们应该使用哪个层面的含义将取决于性能测试的类型。
			- 系统[[吞吐量]]
				- 系统吞吐量，是最能直接体现软件系统负载承受能力的指标。
				- 这里需要注意的是，所有对吞吐量的讨论都必须以“单位时间”作为基本前提。
					- 其实，我认为把“Throughput”翻译成吞吐率更贴切，因为我们可以这样理解：
						- 吞吐率 = 吞吐量 / 单位时间。
						- 但既然国内很多资料已经翻译为了“吞吐量”，所以通常情况下我们不会刻意去区分吞吐量和吞吐率，统称为吞吐量。
				- 对性能测试而言，通常用“Requests/Second”“Pages/Second”“Bytes/Second”来衡量吞吐量。
					- 当然，从业务的角度来讲，吞吐量也可以用单位时间的业务处理数量来衡量。
				- 以不同方式表达的吞吐量可以说明不同层次的问题。
					- 比如：“Bytes/Second”和“Pages/Second”表示的吞吐量，主要受网络设置、服务器架构、应用服务器制约；
					- “Requests/Second”表示的吞吐量，主要受应用服务器和应用本身实现的制约。
				- 这里需要特别注意的是：虽说吞吐量可以反映服务器承受负载的情况，但在不同并发用户数的场景下，即使系统具有相近的吞吐量，但是得到的系统性能瓶颈也会相差甚远。
					- 比如，某个测试场景中采用 100 个并发用户，每个用户每隔 1 秒发出一个 Request，另外一个测试场景采用 1000 个并发用户，每个用户每隔 10 秒发出一个 Request。显然这两个场景具有相同的吞吐量, 都是 100 Requests/second，但是两种场景下的系统性能拐点肯定不同。因为，两个场景所占用的资源是不同的。
				- 这就要求性能测试场景的指标，必然不是单个，需要根据实际情况组合并发用户数、响应时间这两个指标。
	- [[性能测试]]的基本方法与应用领域
	  collapsed:: true
		- 并发用户数、响应时间、系统吞吐量之间的关系
			- 当系统并发用户数较少时，系统的吞吐量也低，系统处于空闲状态，我们往往把这个阶段称为 “空闲区间”。
			- 当系统整体负载并不是很大时，随着系统并发用户数的增长，系统的吞吐量也会随之呈线性增长，我们往往把这个阶段称为 “线性增长区间”。
			- 随着系统并发用户数的进一步增长，系统的处理能力逐渐趋于饱和，因此每个用户的响应时间会逐渐变长。相应地，系统的整体吞吐量并不会随着并发用户数的增长而继续呈线性增长。我们往往把这个阶段称为系统的“拐点”。
			- 随着系统并发用户数的增长，系统处理能力达到过饱和状态。此时，如果继续增加并发用户数，最终所有用户的响应时间会变得无限长。相应地，系统的整体吞吐量会降为零，系统处于被压垮的状态。我们往往把这个阶段称为“过饱和区间”。
		- 常用的七种性能测试方法
			- 后端性能测试
				- 后端性能测试，是通过性能测试工具模拟大量的并发用户请求，然后获取系统性能的各项指标，并且验证各项指标是否符合预期的性能需求的测试手段。
					- 这里的性能指标，
						- 除了包括并发用户数、响应时间和系统吞吐量外，
						- 还应该包括各类资源的使用率，
							- 比如系统级别的 CPU 占用率、内存使用率、磁盘 I/O 和网络 I/O 等，
							- 再比如应用级别以及 JVM 级别的各类资源使用率指标等。
					- 由于需要模拟的并发用户数，通常在“几百”到“几百万”的数量级，所以你选择的性能测试工具，一定不是基于 GUI 的，而是要采用基于协议的模拟方式，也就是去模拟用户在 GUI 操作的过程中实际向后端服务发起的请求。
				- 根据应用领域的不同，后端性能测试的场景设计主要包括以下两种方式：
					- 基于性能需求目标的测试验证；
					- 探索系统的容量，并验证系统容量的可扩展性
			- 前端性能测试
				- 前端性能测试并没有一个严格的定义和标准
				- 通常来讲，前端性能关注的是
					- 浏览器端的页面渲染时间、资源加载顺序、请求数量、前端缓存使用情况、资源压缩等内容，
					- 希望借此找到页面加载过程中比较耗时的操作和资源，然后进行有针对性的优化，最终达到优化终端用户在浏览器端使用体验的目的。
				- 目前，业界普遍采用的前端测试方法，是雅虎（Yahoo）前端团队总结的 7 大类 35 条前端优化规则，你可以通过雅虎网站查看这些规则，以及对各规则的详细解读。
					- https://developer.yahoo.com/performance/rules.html?guccounter=1
				- 我在这里列出了其中几个最典型也是最重要的规则，来帮助你理解前端性能测试优化的关注范围。
					- 减少 http 请求次数：
						- http 请求数量越多，执行过程耗时就越长，所以可以采用合并多个图片到一个图片文件的方法来减少 http 请求次数，也可以采用将多个脚本文件合并成单一文件的方式减少 http 请求次数；
					- 减少 DNS 查询次数：
						- DNS 的作用是将 URL 转化为实际服务器主机 IP 地址，实现原理是分级查找，查找过程需要花费 20~100ms 的时间，所以一方面我们要加快单次查找的时间，另一方面也要减少一个页面中资源使用了多个不同域的情况；
					- 避免页面跳转：
						- 页面跳转相当于又打开一个新的页面，耗费的时间就会比较长，所以要尽量避免使用页面跳转；
					- 使用内容分发网络（CDN）：
						- 使用 CDN 相当于对静态内容做了缓存，并把缓存内容放在网络供应商（ISP）的机房，用户根据就近原则到 ISP 机房获取这些被缓存了的静态资源，因此可以大幅提高性能；
					- Gzip 压缩传输文件：
						- 压缩可以帮助减小传输文件的大小，进而可以从网络传输时间的层面来减少响应时间；
			- 代码级性能测试
				- 代码级性能测试，是指在单元测试阶段就对代码的时间性能和空间性能进行必要的测试和评估，以防止底层代码的效率问题在项目后期才被发现的尴尬。
				- 从实际执行的层面来讲，代码级性能测试并不存在严格意义上的测试工具，通常的做法是：改造现有的单元测试框架。
					- 最常使用的改造方法是：
						-
						  1. 将原本只会执行一次的单元测试用例连续执行 n 次，这个 n 的取值范围通常是 2000~5000；
						-
						  2. 统计执行 n 次的平均时间。如果这个平均时间比较长（也就是单次函数调用时间比较长）的话，比如已经达到了秒级，那么通常情况下这个被测函数的实现逻辑一定需要优化。
					- 这里之所以采用执行 n 次的方式，是因为函数执行时间往往是毫秒级的，单次执行的误差会比较大，所以采用多次执行取平均值的做法。
			- 压力测试
				- 压力测试，通常指的是后端压力测试
					- 一般采用后端性能测试的方法，不断对系统施加压力，并验证系统化处于或长期处于临界饱和阶段的稳定性以及性能指标，并试图找到系统处于临界状态时的主要瓶颈点。
					- 所以，压力测试往往被用于系统容量规划的测试。
					- 还有些情况，在执行压力测试时，我们还会故意在临界饱和状态的基础上继续施加压力，直至系统完全瘫痪，观察这个期间系统的行为；然后，逐渐减小压力，观察瘫痪的系统是否可以自愈。
			- 配置测试
				- 配置测试，主要用于观察系统在不同配置下的性能表现，通常使用后端性能测试的方法：
					-
					  1. 通过性能基准测试（Performance Benchmark）建立性能基线（Performance Baseline）；
					-
					  2. 在此基础上，调整配置；
					-
					  3. 基于同样的性能基准测试，观察不同配置条件下系统性能的差异，根本目的是要找到特定压力模式下的最佳配置。
				- 这里需要注意的是，“配置”是一个广义配置的概念，包含了以下多个层面的配置
					- 宿主操作系统的配置；
					- 应用服务器的配置；
					- 数据库的配置；
					- JVM 的配置；
					- 网络环境的配置；
					- …
			- 并发测试
				- 并发测试，指的是在同一时间，同时调用后端服务，期间观察被调用服务在并发情况下的行为表现，旨在发现诸如资源竞争、资源死锁之类的问题。
				- 集合点并发
					- 为了达到准确控制后端服务并发数的目的，我们需要让某些并发用户到达该集合点时，先处于等待状态，直到参与该集合的全部并发用户都到达时，再一起向后端服务发起请求。
						- 简单地说，就是先到的并发用户要等着，等所有并发用户都到了以后，再集中向后端服务发起请求。
					- 在实际项目中，我建议在要求的并发数上进行适当放大，比如要求的并发数是 100，那我们集合点并发数可以设置为 120。
			- 可靠性测试
				- 可靠性测试，是验证系统在常规负载模式下长期运行的稳定性。
					- 虽然可靠性测试在不同公司的叫法不同，但其本质就是通过长时间模拟真实的系统负载来发现系统潜在的内存泄漏、链接池回收等问题。
				- 由于真实环境下的实际负载，会有高峰和低谷的交替变化（比如，对于企业级应用，白天通常是高峰时段，而晚上则是低峰时段），
					- 所以为了尽可能地模拟出真实的负载情况，我们会每 12 小时模拟一个高峰负载，两个高峰负载中间会模拟一个低峰负载，依次循环 3-7 天，形成一个类似于“波浪形”的系统测试负载曲线。
					- 然后，用这个“波浪形”的测试负载模拟真实的系统负载，完成可靠性测试。同样地，可靠性测试也会持续 3-7 天。
		- [[性能测试]]的四大应用领域
			- 第一，能力验证
				- 能力验证是最常用，也是最容易理解的性能测试的应用领域，主要是验证“某系统能否在 A 条件下具有 B 能力”，通常要求在明确的软硬件环境下，根据明确的系统性能需求设计测试方案和用例。
				- 能力验证这个领域最常使用的测试方法，包括后端性能测试、压力测试和可靠性测试。
			- 第二，能力规划
				- 能力规划关注的是，如何才能使系统达到要求的性能和容量。
					- 通常情况下，我们会采用探索性测试的方式来了解系统的能力。
				- 能力规划解决的问题，主要包括以下几个方面：
					- 能否支持未来一段时间内的用户增长；
					- 应该如何调整系统配置，使系统能够满足不断增长的用户数需求；
					- 应用集群的可扩展性验证，以及寻找集群扩展的瓶颈点；
					- 数据库集群的可扩展性验证；
					- 缓存集群的可扩展性验证；
					- …
				- 能力规划最常使用的测试方法，主要有后端性能测试、压力测试、配置测试和可靠性测试。
			- 第三，性能调优
				- 性能调优，其实是性能测试的延伸。
					- 在一些大型软件公司，会有专门的性能工程（Performance Engineering）团队，除了负责性能测试的工作外，还会负责性能调优。
				- 性能调优主要解决性能测试过程中发现的性能瓶颈的问题，通常会涉及多个层面的调整，包括硬件设备选型、操作系统配置、应用系统配置、数据库配置和应用代码实现的优化等等。
				- 这个领域最常用的测试方法，涵盖了我在上面分享的七大类测试方法，即后端性能测试、前端性能测试、代码级性能测试、压力测试、配置测试、并发测试和可靠性测试。
			- 第四，缺陷发现
				- 缺陷发现，是一个比较直接的应用领域，通过性能测试的各种方法来发现诸如内存泄露、资源竞争、不合理的线程锁和死锁等问题。
				- 缺陷发现，最常用的测试方法主要有并发测试、压力测试、后端性能测试和代码级性能测试。
		- ![image.png](../assets/image_1648368901789_0.png){:height 300, :width 428}
	- [[性能测试]]场景设计
	  collapsed:: true
		- 测试负载组成
			- 虚拟用户脚本
			- 各个虚拟用户脚本的并发数量
			- 总的并发用户数
		- 负载策略
			- 加压策略
			- 减压策略
			- 最大负载运行时间
			- 延时策略
		- 资源监控范围定义
			- 操作系统级别的监控指标
			- 应用服务器的监控指标
			- 数据库服务器的监控指标
			- 缓存集群的监控指标
		- 终止方式
			- 脚本出错时的处理方式
			- 负载熔断机制
		- 负载产生规划
			- 压力产生器数量
			- 网络带宽要求
		- ![image.png](../assets/image_1648369044613_0.png){:height 364, :width 478}
		-
		-
	- 后端性能测试工具的基本原理
	  collapsed:: true
		- 后端性能测试工具首先通过**虚拟用户脚本生成器**生成基于协议的虚拟用户脚本，
		- 然后根据性能测试场景设计的要求，通过**压力控制器**控制协调各个**压力产生器**以并发的方式执行虚拟用户脚本，
		- 并且在测试执行过程中，通过**系统监控器**收集各种性能指标以及系统资源占用率，
		- 最后通过**测试结果分析器**展示测试结果数据。
- 性能工程高手课
	- 性能定律与数理基础
		- 性能工程三定律：IT业和性能优化工作的“法律法规”
			- 帕累托法则
				- 它也被称为 80/20 法则、关键少数法则，或者八二法则
				-