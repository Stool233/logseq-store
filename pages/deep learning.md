- AlexNet
	- https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html
	- 深度神经网络的一个图片训练出来的最后那个向量，在语义空间里的表示特别好。
		- 相似的图片真的会把它放在一起
			- ![image.png](../assets/image_1678613647943_0.png){:height 361, :width 485}
		- 这是一个非常好的特征，非常适合用后面的机器学习，一个简单的分类器就能做的特别好。
			- 这也是深度学习的一大强项
	- 用原始的rgp图片做输入，不需要特征提取。
		- end to end 的卖点
- Transformer
	- ![image.png](../assets/image_1678622969998_0.png){:height 512, :width 365}
	- ![image.png](../assets/image_1678623471454_0.png)
	- 多头注意力机制，有点像在卷积神经网络里面，有多个输出通道的感觉
	- Transfromer与RNN的对比
		- ![image.png](../assets/image_1678625172789_0.png){:height 404, :width 485}
	- 时序信息编码
		- 构建表示位置的向量与输入相加
	- 计算复杂度对比
		- ![image.png](../assets/image_1678625611203_0.png)
		- Attention对模型的假设做了更少，需要更大的模型，更多的数据，才能训练出来。
- GPT
	- 现在没有标号的数据上，训练一个预训练模型（语言模型）
	- 然后在有标号的数据上，微调一个模型
		- 这里是对每一个不同类型的任务，都需要微调一个模型
			- 对于每一个任务，它的输入形式可能需要一些调整
				- 比如添加一些分隔符。
		- 不管输入形式怎么变，输出的构造怎么变，中间的Transfromer模型是不会变的
			- ![image.png](../assets/image_1678628067974_0.png)
- GPT2
	- zero Shot
		- 不微调训练，不需要下游任务的任何标号，也可以得到还不错的结果
	- 问题是，你不能对下游任务的输入形式做任何调整
		- 那么，你的输入形式，必须得是一个自然语言的输入
			- 例如，把英语翻译成法语，可以添加一个prompt：translate to French
				- ![image.png](../assets/image_1678630178330_0.png){:height 266, :width 527}
- GPT3
	- 可以允许用少量有标号的数据，但不是通过梯度下降的参数更新或者fine-tuning
		- in-context learning
			- ![image.png](../assets/image_1678631166311_0.png)
	- 不同参数的对比
		- ![image.png](../assets/image_1678631436797_0.png)
		- 批量的大小很大，对机器的内存有比较大的考验。
		- 对于小的模型来说，批量的大小很大，会导致过拟合。而对于大的模型的来说，这个问题不是那么大。这个业界有一些猜想。
	- 数据收集
		- Common Crawl的数据，然后做质量的过滤。
			- 另外，对于相似文本，文本相似度算法用的是LSH的算法，它可以很快地判断一个集合和另外一个很大的集合之间的相似度
				- 这是Information Retrieval非常常用的技术
	-
	-